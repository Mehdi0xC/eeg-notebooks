{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nSSVEP Analysis\n===============================\n\nThis notebook runs only the data analysis part of N170 notebook.\n\nLook at the notes to see how this can be run on the web with binder or google collab.\n\nAll of the additional notes are removed; only the code cells are kept.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Imports\nfrom muselsl import stream, list_muses, view, record\nfrom multiprocessing import Process\nfrom mne import Epochs, find_events\nfrom time import time, strftime, gmtime\nimport os\n#from stimulus_presentation import n170\n#from eegnb.experiments.visual_n170 import n170\nfrom eegnb.datasets import datasets\nfrom eegnb.analysis import utils\nfrom collections import OrderedDict\nimport warnings\nwarnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Skipping these steps                                                                                                  # ---------------------\n\nStep 1: Connect to an EEG Device\nStep 2: Apply the EEG Device and Wait for Signal Quality to Stabilize\nStep 3: Run the Experiment\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Data\n---------------------\n\nWe will use the\n`MNE sample dataset <https://mne.tools/stable/overview/datasets_index.html?#sample>`_\nwhich is a combined MEG/EEG recording with an audiovisual task.\n\nFirst we will load the dataset from MNE, have a quick look at the data,\nand extract the EEG data that we will use for this example.\n\nNote that if you are running this locally, the following cell will download\nthe example dataset, if you do not already have it.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "eegnb_data_path = os.path.join(os.path.expanduser('~/'),'.eegnb', 'data')    \nssvep_data_path = os.path.join(eegnb_data_path, 'visual-SSVEP', 'eegnb_examples')\n\n# If dataset hasn't been downloaded yet, download it \nif not os.path.isdir(ssvep_data_path):\n  datasets.fetch_dataset(data_dir=eegnb_data_path, experiment='visual-SSVEP', site='eegnb_examples')        \n\n\nsubject = 1\nsession = 1\n\nraw = utils.load_data(ssvep_data_path, sfreq=256., \n                      subject_nb=subject, session_nb=session,\n                      ch_ind=[0, 1, 2, 3, 4], \n                      replace_ch_names={'Right AUX': 'POz'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the power spectrum\n----------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#%matplotlib inline\nraw.plot_psd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Epoching\n----------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Next, we will chunk (epoch) the data into segments representing the data 100ms before to 800ms after each stimulus.\n\n# Note: we will not reject epochs here because the amplitude of the SSVEP at POz is so large it is difficult to separate from eye blinks\n\nevents = find_events(raw)\nevent_id = {'30 Hz': 1, '20 Hz': 2}\nepochs = Epochs(raw, events=events, event_id=event_id, \n                            tmin=-0.5, tmax=4, baseline=None, preload=True,\n                                            verbose=False, picks=[0, 1, 2, 3, 4])\nprint('sample drop %: ', (1 - len(epochs.events)/len(events)) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stimuli-Specific PSD\n----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Next, we can compare the PSD of epochs specifically during 20hz and 30hz stimulus presentation\n\n#%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom mne.time_frequency import psd_welch\nimport numpy as np\n\n\n\nf, axs = plt.subplots(2, 1, figsize=(10, 10))\npsd1, freq1 = psd_welch(epochs['30 Hz'], n_fft=1028, n_per_seg=256 * 3)\npsd2, freq2 = psd_welch(epochs['20 Hz'], n_fft=1028, n_per_seg=256 * 3)\npsd1 = 10 * np.log10(psd1)\npsd2 = 10 * np.log10(psd2)\n\npsd1_mean = psd1.mean(0)\npsd1_std = psd1.mean(0)\n\n\npsd2_mean = psd2.mean(0)\npsd2_std = psd2.mean(0)\n\n\naxs[0].plot(freq1, psd1_mean[[0, 3], :].mean(0), color='b', label='30 Hz')\n\naxs[0].plot(freq2, psd2_mean[[0, 3], :].mean(0), color='r', label='20 Hz')\n\n\naxs[1].plot(freq1, psd1_mean[4, :], color='b', label='30 Hz')\n\naxs[1].plot(freq2, psd2_mean[4, :], color='r', label='20 Hz')\n\n\naxs[0].set_title('TP9 and TP10')\n\naxs[1].set_title('POz')\n\naxs[0].set_ylabel('Power Spectral Density (dB)')\n\naxs[1].set_ylabel('Power Spectral Density (dB)')\n\naxs[0].set_xlim((2, 50))\n\naxs[1].set_xlim((2, 50))\n\naxs[1].set_xlabel('Frequency (Hz)')\n\naxs[0].legend()\n\naxs[1].legend()\n\n\nplt.show();\n\n# With this visualization we can clearly see distinct peaks at 30hz and 20hz in the PSD, corresponding to the frequency of the visual stimulation. The peaks are much larger at the POz electrode, but still visible at TP9 and TP10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Spectrogram\n-----------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We can also look for SSVEPs in the spectrogram, which uses color to represent the power of frequencies in the EEG signal over time\n\nfrom mne.time_frequency import tfr_morlet\n\nfrequencies = np.logspace(1, 1.75, 60)\ntfr, itc = tfr_morlet(epochs['30 Hz'], freqs=frequencies, \n                              n_cycles=15, return_itc=True)\ntfr.plot(picks=[4], baseline=(-0.5, -0.1), mode='logratio', \n                 title='POz - 30 Hz stim');\n\ntfr, itc = tfr_morlet(epochs['20 Hz'], freqs=frequencies, \n                              n_cycles=15, return_itc=True)\ntfr.plot(picks=[4], baseline=(-0.5, -0.1), mode='logratio', \n                 title='POz - 20 Hz stim');\n\nplt.tight_layout()\n\n# Once again we can see clear SSVEPs at 30hz and 20hz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decoding\n----------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We can use a filter bank approach on the original 4 Muse electrodes (to see how the headband alone without external electrodes could be used to classify SSVEP):\n\n#    - Apply bandpass filters around both stimulation frequencies\n#    - Concatenate bandpass-filtered channels\n#    - Extract epochs (from 1 to 3 s after stimulus onset, to avoid classifying the ERP)\n#    - Apply common classification pipelines\n\n# Bandpass filter the raw data\nmuse_raw = raw.drop_channels(['POz'])\nraw_filt_30Hz = muse_raw.copy().filter(25, 35, method='iir')\nraw_filt_20Hz = muse_raw.copy().filter(15, 25, method='iir')\nraw_filt_30Hz.rename_channels(lambda x: x + '_30Hz')\nraw_filt_20Hz.rename_channels(lambda x: x + '_20Hz')\n\n# Concatenate with the bandpass filtered channels\nraw_all = raw_filt_30Hz.add_channels([raw_filt_20Hz], \n                                            force_update_info=True)\n\n# Extract epochs\nevents = find_events(raw_all)\nevent_id = {'30 Hz': 1, '20 Hz': 2}\n\nepochs_all = Epochs(raw_all, events=events, event_id=event_id, tmin=1, \n                             tmax=3, baseline=None, reject={'eeg': 100e-6}, \n                                                  preload=True, verbose=False,)\n\nepochs_all.pick_types(eeg=True)\nX = epochs_all.get_data() * 1e6\ntimes = epochs.times\ny = epochs_all.events[:, -1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decoding\n----------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Next, we will use 4 different machine learning pipelines to classify the SSVEP based on the data we collected. The\n\n# - CSP + RegLDA : Common Spatial Patterns + Regularized Linear Discriminat Analysis. This is a very common EEG analysis pipeline.\n# - Cov + TS : Covariance + Tangent space mapping. One of the most reliable Riemannian geometry-based pipelines.\n# - Cov + MDM: Covariance + MDM. A very simple, yet effective (for low channel count), Riemannian geometry classifier.\n#- CSP + Cov + TS: Common Spatial Patterns + Covariance + Tangent spacem mapping. Riemannian pipeline with the standard CSP procedure beforehand\n\n# Evaluation is done through cross-validation, with area-under-the-curve (AUC) as metric (AUC is probably the best metric for binary and unbalanced classification problem)\n\n# Note: because we're doing machine learning here, the following cell may take a while to complete\n\nimport seaborn as sns\n\nimport pandas as pd\nfrom sklearn.pipeline import make_pipeline\n\nfrom mne.decoding import Vectorizer\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\n\nfrom sklearn.model_selection import cross_val_score, StratifiedShuffleSplit\n\n\nfrom pyriemann.estimation import Covariances, ERPCovariances, XdawnCovariances\n\nfrom pyriemann.spatialfilters import CSP\n\nfrom pyriemann.tangentspace import TangentSpace\n\nfrom pyriemann.classification import MDM\n\n\nfrom collections import OrderedDict\n\n\nclfs = OrderedDict()\n\n\nclfs['CSP + RegLDA'] = make_pipeline(Covariances(), CSP(4), LDA(shrinkage='auto', solver='eigen'))\n\nclfs['Cov + TS'] = make_pipeline(Covariances(), TangentSpace(), LogisticRegression())\n\nclfs['Cov + MDM'] = make_pipeline(Covariances(), MDM())\n\nclfs['CSP + Cov + TS'] = make_pipeline(Covariances(), CSP(4, log=False), TangentSpace(), LogisticRegression())\n\n\n# define cross validation \n\ncv = StratifiedShuffleSplit(n_splits=20, test_size=0.25, \n                                        random_state=42)\n\n\n# run cross validation for each pipeline\n\nauc = []\n\nmethods = []\n\nfor m in clfs:\n\n    print(m)\n     \n    try:\n    \n        res = cross_val_score(clfs[m], X, y==2, scoring='roc_auc', \n                                                                  cv=cv, n_jobs=-1)\n        \n        auc.extend(res)\n        \n        methods.extend([m]*len(res))\n        \n    except:\n    \n        pass\n        \n\n    \nresults = pd.DataFrame(data=auc, columns=['AUC'])\nresults['Method'] = methods\n\nfig = plt.figure(figsize=[8,4])\nsns.barplot(data=results, x='AUC', y='Method')\nplt.xlim(0.4, 1)\nsns.despine()\n\n\n\n# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Blah"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}